# Object Detection

### Datasets

#### COCO

Data can be loaded with PyTorch DataLoader (`torchvision.datasets.CocoDetection`)

```python
torch.multiprocessing.set_sharing_strategy('file_system')
annotation_json = '~/datasets/COCO/annotations_trainval2017/annotations/instances_val2017.json'
dataset1 = dset.CocoDetection(root = '~/datasets/COCO/val2017',
                        annFile = annotation_json,
                        transform=transforms.ToTensor())
dl1 = DataLoader(dataset1, batch_size=1, num_workers=10)
```

##### Data structure of annotation json file

- info: Dataset Info
- licenses: Licences, not important
- images:
  ```json
  [
    {
      "license": 1,
      "file_name": "000000087038.jpg",
      "coco_url": "http://images.cocodataset.org/val2017/000000087038.jpg",
      "height": 480,
      "width": 640,
      "date_captured": "2013-11-14 23:11:37",
      "flickr_url": "http://farm8.staticflickr.com/7355/8825114508_b0fa4d7168_z.jpg",
      "id": 87038
    }
  ]
  ```
- annotations: a list of object detection objects (dict). The details can be found in the next section
- categories
  ```json
  [
    {
      "supercategory": "person",
      "id": 1,
      "name": "person"
    }
  ]
  ```

##### Data structure of detected objects loaded from pytorch dataloader

```json
[
    [
        "image_tensor",
        [
            {
                "segmentation": "",
                "area": "tensor([float])",
                "iscrowd": "tensor([int])",
                "image_id": "tensor([int])",
                "bbox": "[4 x tensor([float])]",
                "category_id": "tensor([int])",
                "id": "tensor([int])"
            }
        ]
    ]
]
```

#### Make Dataset Subset

For each bootstrap iteration, we need to sample some images from the entire dataset. Unlike Pascal VOC which has a single annotation file for every image, COCO stores everything in one json file, causing the file to be huge. After experimenting, I found that I cannot use the same annotation json (original one) for a subset of images. PyTorch dataloader will read every image in the annotation json and try to find it in the given dataset `root`. Since we are making subsets of COCO, the original json won't work.

Given that I can't make a custom dataloader (i.e. I have to use dataloader provided by PyTorch), I will have to make a separate json file containing only the info of selected images.

There are 5 fields in the original json,

- info
- licenses
- images
- annotations
- categories

The data we need is,

1. location of image file
2. annotations for images

After experimenting, I found that it's OK to keep only **images** and **annotations**.

## Dependencies

### detectron2

[detectron2 by facebookresearch](https://github.com/facebookresearch/detectron2) is imported as a submodule.

There are some things you need to do before using it.

1. Since there is no `requirements.txt`, I list some dependencies here.

    ```bash
    pip install numpy
    pip install opencv-python
    pip install tqdm
    pip install fvcore
    pip install cloudpickle
    pip install omegaconf
    # install pytorch based on your OS and hardware
    ```

2. Run `git submodule update --init --update` to download the detectron2 source code
3. Run `python setup.py build develop`
4. If you need to write any code that depends on this in this folder, make sure to add the following code at the top of your python script
    ```python
    import os
    import sys
    root = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, root)
    ```

#### Sample Command

Sample command to run `demo.py` of detectron2.
Change the `--input` image path.

```bash
python demo/demo.py --config-file ./detectron2/configs/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml --input  ~/datasets/COCO/val2017/000000434996.jpg --opts MODEL.WEIGHTS detectron2://COCO-Detection/faster_rcnn_R_50_C4_1x/137257644/model_final_721ade.pkl
```

### Make Patch to detectron2 to save proposal bounding boxes generated by RPN

In the file `object_detection/detectron2/detectron2/modeling/meta_arch/rcnn.py`, add the following code

```python
  np.save(os.path.join(object_detection_root, "object_detection/proposals.npy"), proposals[0]._fields['proposal_boxes'].tensor.clone().cpu().numpy())
  # define object_detection_root to be the folder containing this README.md
```

or run `make patch` to run `patch.sh`, `patch.sh` will add the line automatically.

There may be problems with tabs or spaces, I am using 12 spaces for the new line as the indentation. If you encounter any error, please check the file manually and make sure that the newly added line is using the same type of indentation as the rest of the file. To locate the line, just search for `np.save`

## Bootstrapping

### COCO Bootstrap

I created a `object_detection/run_bootstrap_coco.py` for generating bootstrap images.

Before running it, you need to add image-quality-tools to `object_detection/utils` folder as it's not pushed to GitHub.

I didn't implement a argument parser for it, because adding all arguments in command line could be very long and messy.

Modifying the python file may be cleaner.

Simply change the path to the dataset directory, and annotations json file. The execute the script with python.

If you need to change the number of iterations or sample size for each iteration.



```python
bootstrap_df = bootstrap.bootstrap(df, 5, 5, GAUSSIAN_NOISE, matlab_engine, 1)
```

The second and third parameter corresponds to `num_sample_iter` and `sample_size` respectively.

Just modify them.

The new images will be generated under `object_detection/data/bootstrap`. There will be `iter1`, `iter2` under `bootstrap` folder.

Each iteration folder contains an `images` folder containing all the images and an `annotations.json` file containing the ground truth annotation. The structure is the same as that of the original COCO dataset, so they can be loaded with PyTorch's `torchvision.datasets.CocoDetection`.

`run_bootstrap_coco.py` runs a function `verify_bootstrap_data()` for each bootstrap iteration to make sure the images and annotations are generated properly and can be loaded. If you see no error while running it, then the new data should be good.

### Pascal VOC Bootstrap

A `object_detection/run_bootstrap_pascal_voc.py` is created for generating transformed bootstrap dataset for Pascal VOC dataset. The idea is exactly the same as that of `run_bootstrap_coco.py`.

#### Usage

To run it, simply change the `orig_root` variable in the code. The original dataset root was generated using PyTorch’s `torchvision.datasets.VOCDetection`.

For example, the root should have the following file strucutre

```
> tree -L 3 datasets/PASCAL-VOC
datasets/PASCAL-VOC
├── VOCdevkit
│   └── VOC2007
│       ├── Annotations
│       ├── ImageSets
│       ├── JPEGImages
│       ├── SegmentationClass
│       └── SegmentationObject
└── VOCtrainval_06-Nov-2007.tar
```

#### Explanation

The main logic is in the following code

- Class `PascalVOCDatasetInfo` in `object_detection/src/dataset.py`
- `pascal_voc_save` function in `object_detection/src/bootstrap.py`

## Run Evaluation (COCO)

### Create a Smaller Experiment Dataset

To run evalution with detectron2, you may want to start with a smaller dataset since detectron2 needs to collect result after running inference on the entire dataset first, then the evalution functions could be reached, it could take a very long time. If you just want to experiment, you need to create a smaller dataset.

Here we are using COCO dataset. In order to construct a new smaller dataset, you have to also provide an `annotations.json` file containing all the meta data. Class `CocoDatasetInfo` in `src/datase.py` has a method called `subset_to_json` which takes in a list of image ids and output a json file that's in the same format as the original COCO dataset. I also provide a CLI script you can use to create a subset of COCO, see `tools/coco_subset.py`.

```bash
# Sample
python tools/coco_subset.py -d ~/datasets/coco/val2017 -a ~/datasets/coco/annotations_trainval2017/annotations/instances_val2017.json
```

This script only generates a json file, and doesn't make symbolic links or copies of the images, because when you provide both a annotation json file and a path to the original coco dataset, detectron2 or our program should be able to find the image by concatenating the filename to the image folder root.

You can also run `run_bootstrap_*.py` which creates smaller datasets (transformed).

### Register Dataset

**detectron2** can only load dataset with a dataset name that’s specified in the configuration yaml files that represents a model.

For example,

<img src="README.assets/image-20211115063604102.png" alt="image-20211115063604102" style="zoom:25%;" />

This file represents faster RCNN model. It depends on Base-RCNN-C4.yaml

<img src="README.assets/image-20211115063656709.png" alt="image-20211115063656709" style="zoom:33%;" />

`DATASETS` in this file specify the dataset name. The default value maps to the original coco dataset, and assumes the dataset and annotations to be in a fixed path (very hard to use).

If you need to use a custom dataset, you have to register the dataset with a name and add the name in this yaml file.

```python
from detectron2.data.datasets import register_coco_instances

def update_yaml_dataset_name(new_dataset_name: str):
    with open(Base_RCNN_C4_YAML, 'r') as file:
        dict_file = yaml.full_load(file)
    dict_file['DATASETS']['TEST'] = f'("{new_dataset_name}",)'
    with open(Base_RCNN_C4_YAML, 'w') as file:
        yaml.dump(dict_file, file)

update_yaml_dataset_name(new_dataset_name)
register_coco_instances(new_dataset_name, {},
	str(root / "annotations.json"),
    str(root / "images"))
```

This is the idea. Changing dataset name in yaml file on disk is probably the easiest method. You can also try to edit the loaded yaml file, but that requires you to go edit detectron2’s source code.

I recommend you to write a helper (use the one I wrote) to update yaml config programmatically.

#### A limitation for us

Since we have many iterations of dataset (bootstrap), the yaml has to be updated for each of them with a unique dataset name, because detectron2 doesn’t allow duplicate dataset name or dataset name overwriting, and I could not find a method to un-register a registered dataset.

### Run Prediction Evaluation

`detectron2` has a cli script for running training and evaluation, `tools/train_net.py`

Since we are only interested in evaluation, we will add the `--eval-only` option.

```bash
# sample command
root=~/MVC_reliability
python ./tools/train_net.py \
	--config-file $root/object_detection/detectron2/configs/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml \
    --eval-only \
    MODEL.WEIGHTS detectron2://COCO-Detection/faster_rcnn_R_50_C4_1x/137257644/model_final_721ade.pkl
```

`detectron2/detectron2/evaluation/coco_evaluation.py` contains the main logic for evaluating predictions and bounding boxes.

- `_evaluate_box_proposals` function is for evaluating bounding box proposals generated by RPN, I will talk about this in the next section.
- `_evaluate_predictions` function is for evaluating prediction predictions.

#### How can we get the evaluation results?

If you run the evaluation script `train_net.py`, the results are printed to stdout. The most intuitive method to get these info is to parse the stdout output.

I found a easier method which gives us the results dictionary directly. Since the results are printed out, it must exist somewhere in the code. `train_net.py` started by calling `luanch` function, which doesn’t return anything. If you are running `--eval-only`, the program will only go to the last line of code in `launch` function - `main_func(*arg)`. This `main_func` is actually the `main` function in `train_net.py` and returns the output. All you need to do is capturing the returned value of `main_func` within `launch` function and return it, then you can get it as the output of `launch` function in `train_net.py`.

If you are running any COCO model other than the RPN models, then the result should look like

<img src="README.assets/image-20211115053652970.png" alt="image-20211115053652970" style="zoom:50%;" />

with `bbox` as the key.

#### This is the results of prediction evaluation, what about box proposal evaluation?

<img src="README.assets/image-20211115054033016.png" alt="image-20211115054033016" style="zoom:33%;" />

If we look at The `evaluate` function in `coco_evaluation`, the code will go to `_evaluate_box_proposals` only when `“proposals”` is in the `predictions[0]`’s key. After some investigation with all types of models in the [Model Zoo](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md), it turns out [RPN R50-C4](https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-Detection/rpn_R_50_C4_1x.yaml) and [RPN R50-FPN](https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-Detection/rpn_R_50_FPN_1x.yaml) are the only 2 models that can trigger the `_eval_box_proposals` method, probably because they RPN is designed for box proposal and detectron2 assumed that RPN is the only thing that needs box proposal evaluation.

In theory, RPN is used in a stage of Faster RCNN and should have the data that can be passed to `_eval_box_proposals`. All we need to do is to find the data (box proposals) generated by RPN and insert it into predictions with key “proposals”.

We already know that we can get box proposals in `rcnn.py`, function `inference`. We can do the same thing, save the proposals to disk and read it and add to `predictions[0]` with key “proposals”. After experimenting, this works.

However saving to disk looks pretty “ugly”. Another solution is to save the data in an object which we will get access to during evaluation. Luckily, the `inference` method in `GeneralizedRCNN` in `rcnn.py` is used in evaluation.

I added a `patch` attribute to the `GeneralizedRCNN` class in `rcnn.py`.

<img src="README.assets/image-20211115060245763.png" alt="image-20211115060245763" style="zoom:33%;" />

In `rcnn.py` -> `GeneralizedRCNN` -> `inference()`, add line 209 to save proposals to `self.patch`.

<img src="README.assets/image-20211115060555477.png" alt="image-20211115060555477" style="zoom:50%;" />

In `evaluator.py`, `inference_on_dataset()` function, the `model` is actually a `GeneralizedRCNN` model which now should have a `patch` attribute. After running `model(inputs)`, the proposals should be filled into `patch` already.

If you are running a faster RCNN model, the `outputs` should only contain “instances” key in it. Let’s add proposals to outputs`.

```python
outputs[0]["proposals"] = model.patch["proposals"]
```

<img src="README.assets/image-20211115060206557.png" alt="image-20211115060206557" style="zoom:50%;" />

The `outputs` variable will be the same thing as the `predictions` variable I’ve mentioned about in `coco_evaluation.py`(with a little bit difference in data structure).

Now in `coco_evaluation.py` -> `COCOEvaluator`-> `evaluate()` -> `predictions[0]`, there should be a `proposals` key, and we should be able to enter `_eval_box_proposals`.

<img src="README.assets/image-20211115061536628.png" alt="image-20211115061536628" style="zoom:33%;" />

The final results will be returned together with the evaluation results of predictions.

<img src="README.assets/image-20211115061904058.png" alt="image-20211115061904058" style="zoom:33%;" />

The key is `box_proposals`.

#### A more straight-forward script

I wrote a `eval_exp.py`, so that you don’t need to deal with the `launch` and `main` functions which have lots of unused code for evaluation.

In order to run it, you need to edit the path to `Base_RCNN_YAML`, `args.config_file` and `args.opts` and path to dataset.

I had to use `args` for setting the parameters as detectron2 uses cfg configuration object throughout their code and cfg is generated only from `args` in their code. To make everything simpler, adding parameters to `args` is a quick solution.

I am using a bootstrap data directory which contains multiple iterations. I used a for loop to run evaluate iteration.

`Base_RCNN_YAML`'s dataset name will be updated, since detectron2 doesn’t let me overwrite an existing dataset name.

If you need to run on a single directory, just remove the loop, correctly register your dataset and change dataset name in `Base_RCNN_YAML`.

### Summary

1. In `rcnn.py`, add `self.patch = {}` to `__init__` of `GeneralizedRCNN`, and in `inference()` put proposals into `self.patch`.

2. In `evaluator.py`, `inference_on_dataset()`

   ```python
   outputs[0]["proposals"] = model.patch["proposals"]
   ```

3. Generate your dataset
4. Modify `Base-RCNN-C4.yaml` TEST dataset name.
5. Register your dataset before running anything.



## Run Evaluation (Pascal VOC)

The idea is pretty much the same as COCO, with some exceptions.

When evaluation is run on pascal voc, I received an error (Attribute Error). The error comes from `detectron2/tools/train_net.py`, 

```python 
evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type
```

We expect this line of code to work and go into an `elif` condition in later code

```python
    elif evaluator_type == "pascal_voc":
        return PascalVOCDetectionEvaluator(dataset_name)
```

I couldn’t figure out how to get the `evaluator_type` attribute added to `MetadataCatalog`, so I simply made a patch directly on the variable `evaluator_type`, replace the `evaluator_type` assignment line with the following code.

```python
evaluator_type = "pascal_voc" if "voc" in dataset_name else MetadataCatalog.get(dataset_name).evaluator_type
```

As long as “voc” exists in the dataset name specified in a yaml config file, the `evaluator_type` will be set to “pascal_voc”.

So make sure “voc” is part of the dataset_name and avoid “voc” in any other dataset name such as coco’s configuration.

### Box Proposal Evaluation

The evaluation code for coco is in `detectron2/evaluation/coco_evaluation.py`.

The evaluation code for pascal voc is in `detectron2/evaluation/pascal_voc_evaluation.py`.

This script assumes that you have run `run_bootstrap_pascal_voc.py` to generate the bootstrap images.

What you may want to modify (you may not need to modify anything as there is only one pascal voc config in detectron2):

- Variable `config_file_path`. This file specify the dataset name. This file will be updated every bootstrap iteration. Make sure the right one is used. Open the file and check if `['DATASETS']['TEST']` value is in the file.
- `args.config_file`: very likely the same as `config_file_path`. Get it from model zoo.
- `args.opts`: Model weight
- Variable `voc_orig_root`, this is the pascal voc dataset root. See Bootstrapping/Pascal VOC Section for more details. 



`coco_evaluation.py` contains functions for `_evaluate_box_proposals`, this is why I was able to add box proposal results to the final results (discussed in previous section).

`pascal_voc_evaluation.py` doesn’t contain such function. Although the pascal voc model I experiment with uses `rcnn.py` which will save the intermediate box proposals to model attribute `patch`, it won’t be used later.

